<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Engineering Primer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f7fafc; /* Tailwind gray-100 */
            color: #2d3748; /* Tailwind gray-800 */
        }
        #sidebar {
            min-width: 280px; /* Increased width */
            width: 280px;
            transition: transform 0.3s ease-in-out;
        }
        #sidebar.collapsed {
            transform: translateX(-100%);
        }
        #contentArea h2 {
            font-size: 1.75rem; /* Tailwind text-2xl */
            font-weight: 600; /* Tailwind semibold */
            color: #1a202c; /* Tailwind gray-900 */
            margin-bottom: 1rem;
            border-bottom: 2px solid #e2e8f0; /* Tailwind gray-300 */
            padding-bottom: 0.5rem;
        }
        #contentArea h3 {
            font-size: 1.25rem; /* Tailwind text-xl */
            font-weight: 600; /* Tailwind semibold */
            color: #2d3748; /* Tailwind gray-800 */
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
        }
         #contentArea h4 {
            font-size: 1.1rem; /* Tailwind text-lg */
            font-weight: 600; /* Tailwind semibold */
            color: #2d3748; /* Tailwind gray-800 */
            margin-top: 1.25rem;
            margin-bottom: 0.5rem;
        }
        #contentArea p, #contentArea ul, #contentArea ol {
            margin-bottom: 1rem;
            line-height: 1.75;
            color: #4a5568; /* Tailwind gray-700 */
        }
        #contentArea ul {
            list-style-type: disc;
            padding-left: 1.5rem;
        }
        #contentArea ol {
            list-style-type: decimal;
            padding-left: 1.5rem;
        }
        #contentArea code { /* Inline code */
            background-color: #edf2f7; /* Tailwind gray-200 */
            color: #c53030; /* Tailwind red-700 for emphasis */
            padding: 0.2em 0.4em;
            margin: 0;
            font-size: 0.9em;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
        }
        #contentArea pre { /* Code blocks */
            background-color: #1a202c; /* Tailwind gray-900 */
            color: #f7fafc; /* Tailwind gray-100 */
            padding: 1rem;
            border-radius: 0.375rem; /* Tailwind rounded-md */
            overflow-x: auto;
            margin-bottom: 1rem;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            position: relative; /* For copy button positioning */
        }
        .copy-code-button {
            position: absolute;
            top: 0.5rem;
            right: 0.5rem;
            background-color: #4a5568; /* Tailwind gray-700 */
            color: white;
            border: none;
            padding: 0.25rem 0.5rem;
            font-size: 0.75rem;
            border-radius: 0.25rem;
            cursor: pointer;
            opacity: 0.3; /* Less obtrusive initially */
            transition: opacity 0.2s ease;
        }
        #contentArea pre:hover .copy-code-button {
            opacity: 1;
        }
        .sidebar-link {
            display: block;
            padding: 0.75rem 1.5rem;
            color: #e2e8f0; /* Tailwind gray-300 */
            text-decoration: none;
            border-radius: 0.375rem;
            transition: background-color 0.2s ease, color 0.2s ease;
            font-weight: 500; /* Tailwind medium */
        }
        .sidebar-link:hover, .sidebar-link.active {
            background-color: #4a5568; /* Tailwind gray-700 */
            color: white;
        }
        .sidebar-sublink { /* Not currently used, but available */
            display: block;
            padding: 0.5rem 1.5rem 0.5rem 2.5rem; /* Indented */
            color: #a0aec0; /* Tailwind gray-500 */
            text-decoration: none;
            border-radius: 0.375rem;
            transition: background-color 0.2s ease, color 0.2s ease;
            font-size: 0.9rem;
        }
        .sidebar-sublink:hover, .sidebar-sublink.active {
            background-color: #2d3748; /* Tailwind gray-800 */
            color: #e2e8f0; /* Tailwind gray-300 */
        }

        .nav-buttons-container {
            display: flex;
            justify-content: space-between;
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #e2e8f0; /* Tailwind gray-300 */
        }
        .nav-button {
            background-color: #4299e1; /* Tailwind blue-500 */
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.2s ease;
        }
        .nav-button:hover {
            background-color: #2b6cb0; /* Tailwind blue-700 */
        }
        .nav-button.disabled {
            background-color: #a0aec0; /* Tailwind gray-500 */
            cursor: not-allowed;
        }
        .toast {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%) translateY(100%);
            padding: 12px 24px;
            border-radius: 6px;
            color: white;
            font-size: 1em;
            z-index: 1000;
            opacity: 0;
            transition: opacity 0.3s ease-in-out, transform 0.3s ease-in-out;
            box-shadow: 0 2px 10px rgba(0,0,0,0.2);
        }
        .toast.show {
            opacity: 1;
            transform: translateX(-50%) translateY(0);
        }
        .toast.success {
            background-color: #38a169; /* Tailwind green-600 */
        }
        .note-box { /* For callouts/notes */
            background-color: #ebf8ff; /* Tailwind blue-100 */
            border-left: 4px solid #4299e1; /* Tailwind blue-500 */
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 0.25rem;
        }
        .note-box p {
            color: #2c5282; /* Tailwind blue-800 */
            margin-bottom: 0.5rem !important; /* Override general p margin */
        }
        .note-box p:last-child {
            margin-bottom: 0 !important;
        }
    </style>
</head>
<body class="flex h-screen overflow-hidden">

    <aside id="sidebar" class="bg-gray-800 text-gray-100 p-6 space-y-2 overflow-y-auto h-full shadow-lg">
        <h1 class="text-2xl font-bold text-white mb-6 border-b border-gray-700 pb-3">Primer Sections</h1>
        <nav id="sidebarNav">
            </nav>
    </aside>

    <button id="menuToggle" class="fixed top-4 left-4 z-50 p-2 bg-gray-800 text-white rounded-md md:hidden">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
        </svg>
    </button>

    <main class="flex-1 p-6 md:p-10 overflow-y-auto bg-white">
        <div id="contentArea">
            </div>
        <div id="navButtonsContainer" class="nav-buttons-container">
            <button id="prevButton" class="nav-button">Previous</button>
            <button id="nextButton" class="nav-button">Next</button>
        </div>
    </main>

    <div id="toastNotification" class="toast"></div>

    <script>
        const primerData = [
            {
                id: "introduction",
                title: "Introduction to Prompt Engineering",
                content: `
                    <h2>What is Prompt Engineering?</h2>
                    <p>Prompt engineering is the art and science of crafting effective inputs (prompts) to guide Large Language Models (LLMs) like ChatGPT, Claude, Gemini, etc., towards generating desired, accurate, and useful outputs. It's about communicating your intent to the AI in the clearest and most effective way possible.</p>
                    <p>Think of an LLM as an incredibly knowledgeable and versatile assistant. However, like any assistant, it needs clear instructions to perform tasks well. The quality of the output you get from an LLM is directly proportional to the quality of the prompt you provide.</p>

                    <h3>Why Does it Matter for Knowledge Workers?</h3>
                    <p>For knowledge workers, mastering prompt engineering can lead to significant benefits:</p>
                    <ul>
                        <li><strong>Increased Efficiency:</strong> Get the information or content you need faster, reducing time spent on research, drafting, and revision.</li>
                        <li><strong>Improved Output Quality:</strong> Generate more accurate, relevant, and nuanced responses from AI tools.</li>
                        <li><strong>Enhanced Creativity:</strong> Use LLMs as brainstorming partners, idea generators, or to explore different perspectives.</li>
                        <li><strong>Task Automation:</strong> Automate repetitive tasks like summarizing documents, drafting emails, or generating code snippets.</li>
                        <li><strong>Unlocking AI Potential:</strong> Move beyond simple Q&A to leverage the full power of LLMs for complex tasks.</li>
                    </ul>

                    <h3>How This Primer is Structured</h3>
                    <p>This primer will guide you through the foundational concepts of prompt engineering, explore various techniques, provide practical examples, and highlight common pitfalls. Our goal is to equip you with the knowledge to write better prompts and get more value from AI language models in your daily work.</p>
                    <p>Navigate through the sections using the sidebar. Each section builds upon the previous ones, but feel free to jump to topics that interest you most.</p>
                `
            },
            {
                id: "core-concepts",
                title: "Core Concepts of Prompting",
                content: `
                    <h2>Understanding the Fundamentals</h2>
                    <p>Effective prompting hinges on a few core principles. Mastering these will dramatically improve your interactions with any LLM.</p>
                    
                    <h3>1. Clarity & Specificity: The Golden Rule</h3>
                    <p>The most crucial aspect of a good prompt is being crystal clear and highly specific about what you want. Ambiguity is the enemy of good AI output.</p>
                    <ul>
                        <li><strong>Vague Prompt:</strong> <code>"Tell me about dogs."</code> (This could result in anything from dog breeds to dog behavior to famous dogs in history.)</li>
                        <li><strong>Specific Prompt:</strong> <code>"Provide a summary of the key characteristics of the Labrador Retriever breed, including temperament, common health issues, and average lifespan. Present this as a bulleted list."</code></li>
                    </ul>
                    <p>The more precise your instructions, the better the AI can understand and fulfill your request.</p>

                    <h3>2. Context is Key</h3>
                    <p>LLMs don't inherently know the background of your request unless you provide it. Giving relevant context helps the AI tailor its response appropriately.</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>Without Context: "Draft an email to the team."

With Context: "Our team just completed a major project milestone (Project Phoenix Phase 1). Draft an email to the team (John, Sarah, David) congratulating them on their hard work and success. Mention the tight deadline and how everyone pulled together."</code></pre>
                    <p>The second prompt will yield a much more relevant and meaningful email.</p>

                    <h3>3. Defining the AI's Role/Persona</h3>
                    <p>You can instruct the AI to adopt a specific role or persona. This helps shape the tone, style, and even the type of knowledge it draws upon.</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Act as an expert financial analyst. Explain the concept of compound interest to a beginner investor."

"You are a witty and sarcastic comedian. Write a short monologue about the frustrations of online dating."</code></pre>

                    <h3>4. Specifying Output Format</h3>
                    <p>Clearly state how you want the information presented. This saves you time reformatting the AI's output.</p>
                    <p><strong>Examples:</strong></p>
                    <ul>
                        <li><code>"Provide the answer as a bulleted list."</code></li>
                        <li><code>"Write the summary in three short paragraphs."</code></li>
                        <li><code>"Generate a table comparing features A, B, and C."</code></li>
                        <li><code>"Output the code in Python."</code></li>
                        <li><code>"Present the information in a JSON format with keys 'name', 'description', and 'price'."</code></li>
                    </ul>

                    <h3>5. Setting Constraints</h3>
                    <p>Guide the AI by setting limitations or boundaries for its response.</p>
                    <p><strong>Examples:</strong></p>
                    <ul>
                        <li><code>"Keep the explanation under 200 words."</code></li>
                        <li><code>"Use a formal and professional tone."</code></li>
                        <li><code>"Avoid using technical jargon."</code></li>
                        <li><code>"Do not mention specific competitor names."</code></li>
                    </ul>

                    <h3>6. Iterative Refinement</h3>
                    <p>Your first prompt might not always produce the perfect result. Prompt engineering is often an iterative process. Analyze the AI's output, identify areas for improvement, and refine your prompt accordingly.</p>
                    <p>Don't be afraid to: </p>
                    <ul>
                        <li>Ask follow-up questions.</li>
                        <li>Request clarifications or elaborations.</li>
                        <li>Tell the AI what it did well and what it should change.</li>
                        <li>Break down a complex request into smaller steps if the initial output is off-track.</li>
                    </ul>
                `
            },
            {
                id: "basic-techniques",
                title: "Basic Prompting Techniques",
                content: `
                    <h2>Getting Started with Prompting Techniques</h2>
                    <p>Beyond the core concepts, several foundational techniques can help you structure your prompts effectively.</p>

                    <h3>1. Zero-Shot Prompting</h3>
                    <p>This is the simplest form of prompting. You ask the LLM to perform a task without providing any prior examples of how to do it. The model relies entirely on its pre-trained knowledge.</p>
                    <p><strong>When to use:</strong> For straightforward tasks where the AI likely has sufficient training data (e.g., simple summarization, general knowledge questions, basic text generation).</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Translate the following English sentence to French: 'Hello, how are you today?'"

"What is the capital of Australia?"</code></pre>

                    <h3>2. Few-Shot Prompting</h3>
                    <p>In few-shot prompting, you provide the LLM with a small number (typically 1 to 5) of examples (or "shots") of the task you want it to perform. These examples help the model understand the desired format, style, and type of output.</p>
                    <p><strong>When to use:</strong> When the task is more nuanced, requires a specific output structure, or when zero-shot prompting doesn't yield good results. It's very effective for tasks like classification, sentiment analysis, or generating text in a particular style.</p>
                    <p><strong>Example (Sentiment Classification):</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>This is awesome! // Positive
This is bad. // Negative
Wow, I love this! // Positive
What a terrible experience. // Negative
The movie was okay. // [AI should classify this, e.g., Neutral]</code></pre>
                    <p><strong>Example (Generating Taglines):</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>Product: Eco-friendly water bottle
Tagline: Sip Sustainably.

Product: Noise-cancelling headphones
Tagline: Your World, Your Sound.

Product: AI-powered project management tool
Tagline: [AI should generate a tagline here]</code></pre>
                    <p>The quality of your examples is crucial for few-shot prompting to be effective.</p>

                    <h3>3. Instruction Prompting</h3>
                    <p>This is a direct and explicit way of telling the AI what to do. It involves using clear action verbs and detailed instructions.</p>
                    <p><strong>When to use:</strong> For almost any task. It often forms the basis of more complex prompts.</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Write a three-paragraph blog post about the benefits of remote work for employee productivity. Start with an engaging hook and conclude with a call to action for managers to consider flexible work arrangements."

"Generate a list of 5 potential names for a new coffee shop that has a cozy, vintage theme."</code></pre>

                    <h3>4. Role Prompting (Revisited)</h3>
                    <p>As mentioned in Core Concepts, assigning a role to the AI is a powerful technique. It helps set the context, tone, style, and even the knowledge base the AI should draw from.</p>
                    <p><strong>Crafting Effective Personas:</strong></p>
                    <ul>
                        <li><strong>Be Specific:</strong> Instead of "Act as a writer," try "Act as a seasoned travel writer specializing in budget backpacking adventures in Southeast Asia."</li>
                        <li><strong>Define Expertise:</strong> "You are an expert in 17th-century European history."</li>
                        <li><strong>Specify Perspective:</strong> "Respond from the perspective of a skeptical investor."</li>
                        <li><strong>Combine with Task:</strong> "Act as a friendly customer service representative. A customer is upset because their order is late. Draft a polite and empathetic response that explains the delay (due to unforeseen shipping issues) and offers a small discount on their next purchase."</li>
                    </ul>
                    <p>Role prompting can significantly improve the relevance and quality of the AI's output by aligning it more closely with your expectations.</p>
                `
            },
            {
                id: "advanced-techniques",
                title: "Intermediate & Advanced Techniques",
                content: `
                    <h2>Level Up Your Prompting Skills</h2>
                    <p>Once you're comfortable with the basics, explore these more advanced techniques to elicit even better responses from LLMs.</p>

                    <h3>1. Chain-of-Thought (CoT) / Step-by-Step Thinking</h3>
                    <p>Chain-of-Thought prompting encourages the LLM to break down a complex problem into intermediate reasoning steps before arriving at a final answer. You essentially ask the AI to "think out loud."</p>
                    <p><strong>How it works:</strong> You can prompt this by adding phrases like "Let's think step by step," or by providing few-shot examples where the reasoning process is explicitly shown.</p>
                    <p><strong>Benefits:</strong> Significantly improves performance on tasks requiring arithmetic, commonsense, or symbolic reasoning. It also makes the AI's reasoning process more transparent.</p>
                    <p><strong>Example (Simple Math Problem):</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: Roger started with 5 balls.
He bought 2 cans, and each can has 3 balls, so that's 2 * 3 = 6 new balls.
In total, Roger now has 5 + 6 = 11 tennis balls.
The final answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?
A: [AI should show its reasoning steps here]</code></pre>

                    <h3>2. Self-Consistency</h3>
                    <p>This technique builds upon Chain-of-Thought. Instead of just taking the first reasoning path, you prompt the LLM multiple times (perhaps with slightly different phrasing or by using a higher "temperature" setting if available, which encourages more varied outputs) to generate several different reasoning paths. Then, you select the most common answer among these paths.</p>
                    <p><strong>Benefits:</strong> Improves the robustness and accuracy of answers for complex reasoning tasks, as it's less likely that multiple different reasoning paths will arrive at the same incorrect answer.</p>
                    <p><strong>How to apply:</strong> This is more of a meta-technique for the user. You would run the same CoT prompt multiple times and then analyze the outputs to find the consensus answer.</p>

                    <h3>3. Generated Knowledge Prompting</h3>
                    <p>This technique involves first prompting the LLM to generate relevant facts or knowledge about a topic, and then using that generated knowledge as part of the context for a subsequent prompt to answer the main question.</p>
                    <p><strong>Benefits:</strong> Helps the LLM access and utilize more specific and relevant information, especially for questions where it might not immediately retrieve the necessary details.</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>Prompt 1: "Generate 3 key facts about the economic impact of the printing press in Europe."

(AI generates facts)

Prompt 2: "Based on the following facts: [Paste AI-generated facts here]. Write a short paragraph explaining how the printing press revolutionized European society."</code></pre>

                    <h3>4. Retrieval Augmented Generation (RAG) - Conceptual Overview</h3>
                    <p>RAG is a more advanced system architecture where the LLM's knowledge is augmented with information retrieved from external sources (like a specific database of documents or a section of the internet) at the time of prompting.</p>
                    <p><strong>Conceptual Application for Users:</strong> While you might not build a full RAG system, the principle is valuable. If an LLM supports file uploads or if you can paste text, providing the LLM with specific documents or text passages and asking it to answer questions *based solely on that provided text* is a form of simple RAG. This grounds the AI's answers in your specific information and dramatically reduces hallucinations.</p>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Referencing only the provided company policy document below, answer the following question: What is the company's policy on remote work?

Company Policy Document:
---
[Paste relevant sections of the company policy here]
---

Question: What is the company's policy on remote work?"</code></pre>

                    <h3>5. Using Delimiters</h3>
                    <p>Delimiters are characters or strings used to clearly separate different parts of your prompt, such as instructions, context, examples, input data, and the question.</p>
                    <p><strong>Benefits:</strong> Helps the LLM understand the structure of your prompt, especially for complex requests with multiple components. This reduces ambiguity and improves the model's ability to follow instructions.</p>
                    <p><strong>Common Delimiters:</strong></p>
                    <ul>
                        <li>Triple quotes: <code>"""Text"""</code></li>
                        <li>Triple backticks: <code>\`\`\`Text\`\`\`</code></li>
                        <li>Triple dashes: <code>---</code></li>
                        <li>Angle brackets (like XML tags): <code>&lt;context&gt;Text&lt;/context&gt;</code></li>
                        <li>Section headers: <code>###Instruction###</code>, <code>###Context###</code></li>
                    </ul>
                    <p><strong>Example:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>###Instruction###
Summarize the following text in three bullet points.

###Text###
[Paste your long text here]

###Summary###
[AI generates summary here]</code></pre>

                    <h3>6. Negative Prompts / Constraints (Emphasized)</h3>
                    <p>Explicitly telling the AI what *not* to do, what topics to avoid, or what constraints to adhere to can be just as important as telling it what to do.</p>
                    <p><strong>Benefits:</strong> Provides finer control over the output, helps avoid undesirable content or styles, and ensures the response stays within defined boundaries.</p>
                    <p><strong>Examples:</strong></p>
                    <ul>
                        <li><code>"Write a product description for our new software. Do not use marketing jargon or buzzwords. Avoid mentioning the price."</code></li>
                        <li><code>"Explain the concept of photosynthesis. Do not go into the biochemical details of the Calvin cycle."</code></li>
                        <li><code>"Generate a list of team-building activities. Ensure all activities are suitable for a remote team and require no specialized equipment."</code></li>
                    </ul>
                `
            },
            {
                id: "crafting-guide",
                title: "Crafting Effective Prompts: Guide",
                content: `
                    <h2>A Practical Approach to Prompt Design</h2>
                    <p>Crafting truly effective prompts is an iterative skill that combines understanding the LLM's capabilities with clear communication. Here’s a practical guide:</p>

                    <h3>1. Start with a Clear Goal</h3>
                    <p>Before you write a single word of your prompt, ask yourself: <strong>What exactly do I want the AI to achieve?</strong> A well-defined objective is the foundation of a good prompt.</p>
                    <ul>
                        <li>Is it to generate text (email, report, code)?</li>
                        <li>Is it to summarize information?</li>
                        <li>Is it to answer a question?</li>
                        <li>Is it to brainstorm ideas?</li>
                        <li>Is it to translate language?</li>
                    </ul>
                    <p>The more specific your goal, the easier it will be to structure your prompt.</p>

                    <h3>2. Know Your Audience (for the AI's Output)</h3>
                    <p>Consider who will be reading or using the AI's generated response. This will influence the tone, style, complexity, and vocabulary you ask the AI to use.</p>
                    <ul>
                        <li><strong>Example for executives:</strong> <code>"Summarize this financial report for a non-technical executive audience, focusing on key takeaways and strategic implications. Keep it under 500 words."</code></li>
                        <li><strong>Example for technical peers:</strong> <code>"Explain the architecture of this microservice, detailing the communication protocols between components and potential scaling bottlenecks."</code></li>
                    </ul>

                    <h3>3. Break Down Complex Tasks</h3>
                    <p>If you have a large or complex request, don't try to cram it all into one massive prompt. LLMs perform better when tasks are broken down into smaller, more manageable steps. You can then chain these prompts together, using the output of one as input or context for the next.</p>
                    <p><strong>Example: Writing a research report</strong></p>
                    <ol>
                        <li>Prompt 1: "Generate an outline for a research report on [topic], including sections for Introduction, Literature Review, Methodology, Findings, Discussion, and Conclusion."</li>
                        <li>Prompt 2 (for each section): "Write the Introduction section for a research report on [topic], based on the following key points: [point A], [point B]."</li>
                        <li>Prompt 3: "Review the following draft of a research report for clarity, coherence, and grammatical errors. Suggest improvements. Draft: [text]"</li>
                    </ol>

                    <h3>4. Provide High-Quality Examples (for Few-Shot Prompting)</h3>
                    <p>When using few-shot prompting, the quality and relevance of your examples are paramount.</p>
                    <ul>
                        <li>Ensure your examples accurately reflect the desired output format, style, and content.</li>
                        <li>Use diverse examples if you want the AI to generalize well.</li>
                        <li>Make sure the examples are correct and well-written.</li>
                    </ul>

                    <h3>5. Be Explicit with Instructions</h3>
                    <p>Don't assume the AI knows what you mean. Spell out your requirements clearly.</p>
                    <ul>
                        <li>Instead of: "Make it shorter." Try: "Summarize this in 3 key bullet points."</li>
                        <li>Instead of: "Write about our product." Try: "Write a 100-word product description for [Product Name] highlighting its top 3 benefits for [Target Customer]."</li>
                    </ul>

                    <h3>6. Test and Iterate: The Core Loop</h3>
                    <p>Prompt engineering is rarely a one-shot success. Expect to refine your prompts.</p>
                    <div class="note-box">
                        <p><strong>The Iterative Prompting Cycle:</strong></p>
                        <ol>
                            <li><strong>Draft Prompt:</strong> Write your initial prompt based on your goal.</li>
                            <li><strong>Generate Output:</strong> Run the prompt with the LLM.</li>
                            <li><strong>Analyze Output:</strong>
                                <ul>
                                    <li>Is it accurate?</li>
                                    <li>Is it relevant to your goal?</li>
                                    <li>Is it complete?</li>
                                    <li>Is it in the correct format, tone, and style?</li>
                                    <li>Are there any hallucinations, biases, or unwanted content?</li>
                                </ul>
                            </li>
                            <li><strong>Refine Prompt:</strong> Based on the analysis, modify your prompt. Consider:
                                <ul>
                                    <li>Adding more specificity or clarity.</li>
                                    <li>Providing more (or better) context.</li>
                                    <li>Changing or refining the persona.</li>
                                    <li>Adding or adjusting constraints.</li>
                                    <li>Adding few-shot examples.
                                    <li>Breaking the task into smaller sub-prompts.</li>
                                </ul>
                            </li>
                            <li><strong>Repeat:</strong> Go back to step 2 until you achieve the desired output.</li>
                        </ol>
                    </div>
                    <p>This iterative process is key to mastering prompt engineering and getting the best results from LLMs.</p>
                `
            },
            {
                id: "specific-tasks",
                title: "Prompting for Specific Tasks",
                content: `
                    <h2>Examples for Common Knowledge Worker Needs</h2>
                    <p>Let's look at how to apply prompting principles to tasks frequently encountered by knowledge workers. We'll use a "Problem -> Less Effective Prompt -> More Effective Prompt -> Explanation" format.</p>

                    <h3>1. Summarization</h3>
                    <p><strong>Problem:</strong> You need to quickly grasp the main points of a long document.</p>
                    <p><strong>Less Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Summarize this article: [Paste Article Text]"</code></pre>
                    <p><strong>More Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Act as a research assistant. Summarize the following academic article for a busy executive who needs to understand its key findings and business implications. Provide the summary in three concise bullet points, followed by a one-sentence takeaway.
Article:
---
[Paste Article Text]
---"</code></pre>
                    <p><strong>Explanation:</strong> The "More Effective Prompt" specifies a role (research assistant), the target audience for the summary (busy executive), the desired output format (3 bullet points + takeaway), and the focus (key findings, business implications). It also uses delimiters for clarity.</p>

                    <h3>2. Information Extraction</h3>
                    <p><strong>Problem:</strong> You need to find specific pieces of information within a large block of text.</p>
                    <p><strong>Less Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Find the important stuff in this meeting transcript: [Paste Transcript]"</code></pre>
                    <p><strong>More Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"From the provided meeting transcript below, extract the following information:
1. All decisions made.
2. All action items, including the person responsible for each and their due dates.
3. Key unresolved questions or open issues.
Present this information in a structured format, using clear headings for each category.

Meeting Transcript:
---
[Paste Meeting Transcript]
---"</code></pre>
                    <p><strong>Explanation:</strong> The "More Effective Prompt" clearly lists what information to extract and specifies a structured output format, making the AI's job easier and the result more usable.</p>

                    <h3>3. Drafting & Writing (e.g., Email)</h3>
                    <p><strong>Problem:</strong> You need to compose a professional email.</p>
                    <p><strong>Less Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Write an email to John about the project."</code></pre>
                    <p><strong>More Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Draft a professional and polite follow-up email to John Smith (john.smith@example.com) regarding the 'Alpha Project' proposal we discussed last Tuesday.
Key points to include:
- Thank him for his time.
- Briefly reiterate the main benefit of our proposal for his company (increased efficiency by 15%).
- Ask if he has had a chance to review the proposal and if he has any questions.
- Suggest a brief call next week to discuss further, offering two specific time slots: [Your Time Slot 1, e.g., Tuesday at 10 AM EST] or [Your Time Slot 2, e.g., Thursday at 2 PM EST].
Maintain a confident but not pushy tone.
My name is [Your Name], [Your Title]."</code></pre>
                    <p><strong>Explanation:</strong> This prompt provides extensive context: recipient, subject, previous interaction, key message points, desired tone, and specific call to action details.</p>

                    <h3>4. Brainstorming & Idea Generation</h3>
                    <p><strong>Problem:</strong> You need fresh ideas for a new initiative.</p>
                    <p><strong>Less Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Give me some marketing ideas."</code></pre>
                    <p><strong>More Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Act as an innovative marketing strategist. Brainstorm 5 unconventional marketing campaign ideas for a new brand of sustainable, ethically sourced coffee. The target audience is environmentally conscious millennials (ages 25-35). For each idea, provide:
1. A catchy campaign name/slogan.
2. A brief description of the core concept (2-3 sentences).
3. The primary marketing channel(s) you'd recommend for this idea.
Focus on ideas that emphasize community and authenticity."</code></pre>
                    <p><strong>Explanation:</strong> Assigning a role, defining the product, target audience, specific output requirements (number of ideas, components for each idea), and desired themes (community, authenticity) leads to much richer and more relevant brainstorming output.</p>

                    <h3>5. Learning & Explanation</h3>
                    <p><strong>Problem:</strong> You need to understand a complex concept quickly.</p>
                    <p><strong>Less Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Explain blockchain."</code></pre>
                    <p><strong>More Effective Prompt:</strong></p>
                    <pre><button class="copy-code-button">Copy</button><code>"Explain the core concept of 'blockchain technology' as if you were talking to a business manager with no technical background in the subject. Use a simple analogy to illustrate how it works. Focus on its key benefits like security and transparency, and keep the explanation under 250 words. Avoid highly technical jargon."</code></pre>
                    <p><strong>Explanation:</strong> Specifying the target audience for the explanation (business manager, no tech background), requesting an analogy, highlighting key aspects to cover, and setting a length constraint helps the AI produce a tailored and understandable explanation.</p>
                `
            },
            {
                id: "common-pitfalls",
                title: "Common Pitfalls & Avoidance",
                content: `
                    <h2>Mistakes to Watch Out For</h2>
                    <p>Even with good intentions, it's easy to fall into common traps when writing prompts. Being aware of these pitfalls can help you avoid them and get better results more consistently.</p>

                    <h3>1. Vague or Ambiguous Prompts</h3>
                    <p><strong>Pitfall:</strong> Using unclear language, undefined pronouns, or overly broad requests.</p>
                    <p><strong>Example:</strong> <code>"Write about it."</code> (What is "it"? What kind of writing?)</p>
                    <p><strong>Avoidance:</strong> Always strive for maximum clarity and specificity. Define your terms, explicitly state your subject, and detail your requirements.</p>

                    <h3>2. Lack of Sufficient Context</h3>
                    <p><strong>Pitfall:</strong> Assuming the AI knows background information that it hasn't been given.</p>
                    <p><strong>Example:</strong> <code>"Summarize our last meeting."</code> (The AI wasn't in your meeting!)</p>
                    <p><strong>Avoidance:</strong> Provide all necessary context directly in the prompt. This includes relevant background, previous discussions, specific data, or any constraints the AI needs to be aware of.</p>

                    <h3>3. Overly Complex Prompts (Too Many Tasks at Once)</h3>
                    <p><strong>Pitfall:</strong> Asking the AI to perform multiple, distinct, and complex tasks within a single prompt.</p>
                    <p><strong>Example:</strong> <code>"Write a blog post about AI, then create a social media campaign for it with 5 tweets and 3 LinkedIn posts, also draft a follow-up email to potential collaborators, and suggest a title for the webinar series we'll do on this."</code></p>
                    <p><strong>Avoidance:</strong> Break down complex requests into a series of smaller, more focused prompts. Address one primary goal per prompt for better quality and control.</p>

                    <h3>4. Not Specifying Desired Format, Length, or Tone</h3>
                    <p><strong>Pitfall:</strong> Leaving the output structure and style entirely up to the AI, which can lead to responses that are too long, too short, in the wrong format, or have an inappropriate tone.</p>
                    <p><strong>Example:</strong> <code>"Explain photosynthesis."</code> (Could be a single sentence or a 10-page essay.)</p>
                    <p><strong>Avoidance:</strong> Explicitly state your requirements: <code>"Explain photosynthesis in three paragraphs suitable for a middle school student, using a friendly and engaging tone."</code></p>

                    <h3>5. Ignoring the AI's Tendency to "Hallucinate"</h3>
                    <p><strong>Pitfall:</strong> Trusting every piece of information the LLM generates as factual, especially for topics where it might have limited or outdated training data.</p>
                    <p><strong>Avoidance:</strong> Always critically evaluate the AI's output, especially for factual claims, statistics, or critical information. Cross-verify with reliable sources. For critical tasks, ask the AI to cite sources if it has that capability, or provide it with trusted source material to work from (RAG concept).</p>

                    <h3>6. Bias in Prompts Leading to Biased Outputs</h3>
                    <p><strong>Pitfall:</strong> Phrasing prompts in a way that introduces or reinforces societal biases (gender, race, etc.), leading the AI to generate biased or stereotyped responses.</p>
                    <p><strong>Avoidance:</strong> Be mindful of your language. Strive for neutral phrasing. If you suspect bias in an output, try rephrasing the prompt, asking for multiple perspectives, or explicitly instructing the AI to avoid stereotypes.</p>

                    <h3>7. Leading Questions</h3>
                    <p><strong>Pitfall:</strong> Asking questions in a way that suggests a desired answer, which can prevent the AI from providing a more objective or comprehensive response.</p>
                    <p><strong>Example:</strong> <code>"Don't you think our new product is the best on the market and will revolutionize the industry?"</code></p>
                    <p><strong>Avoidance:</strong> If you want an unbiased assessment or diverse ideas, ask open-ended, neutral questions: <code>"Analyze the potential market impact of our new product compared to existing solutions."</code></p>

                    <h3>8. Assuming Prior Knowledge or Memory (in a conversation)</h3>
                    <p><strong>Pitfall:</strong> Expecting the LLM to perfectly remember all details from previous turns in a long conversation, especially for models with limited context windows.</p>
                    <p><strong>Avoidance:</strong> In longer interactions, or when starting a new but related query, briefly re-state key context or important information from earlier in the conversation to ensure the AI is on the right track.</p>

                    <h3>9. Not Iterating or Giving Up Too Soon</h3>
                    <p><strong>Pitfall:</strong> Getting a suboptimal response from the first prompt and assuming the AI isn't capable of the task.</p>
                    <p><strong>Avoidance:</strong> Embrace iterative refinement. Analyze the output, identify what's missing or wrong, and adjust your prompt. Often, a few tweaks can lead to dramatically better results.</p>
                `
            },
            {
                id: "glossary",
                title: "Glossary of Terms",
                content: `
                    <h2>Key Prompt Engineering Terminology</h2>
                    <p>Understanding these common terms will help you navigate the world of prompt engineering and LLMs more effectively.</p>
                    <ul>
                        <li><strong>Prompt:</strong> The input text, question, or instruction given to a Large Language Model (LLM) to elicit a response.</li>
                        <li><strong>LLM (Large Language Model):</strong> An advanced artificial intelligence model trained on vast amounts of text data to understand, generate, and manipulate human language (e.g., GPT-3/4, Claude, Gemini, Llama).</li>
                        <li><strong>Zero-Shot Prompting:</strong> Asking an LLM to perform a task without providing any prior examples of how to do it.</li>
                        <li><strong>Few-Shot Prompting:</strong> Providing the LLM with a small number of examples (1-5 "shots") of the desired input-output behavior to guide its response for a new input.</li>
                        <li><strong>Chain-of-Thought (CoT) Prompting:</strong> A technique where the LLM is prompted to generate intermediate reasoning steps before arriving at a final answer, improving performance on complex tasks.</li>
                        <li><strong>Persona / Role Prompting:</strong> Instructing the LLM to adopt a specific character, role, or expertise (e.g., "Act as an expert historian...").</li>
                        <li><strong>Context Window:</strong> The amount of text (input prompt + generated output) that an LLM can consider at one time. Information outside this window might be "forgotten" by the model in a long conversation.</li>
                        <li><strong>Temperature:</strong> A parameter (often in LLM APIs or playgrounds, typically 0 to 1 or higher) that controls the randomness of the AI's output. Higher temperatures (e.g., 0.7-1.0) result in more creative or diverse responses, while lower temperatures (e.g., 0.1-0.3) produce more focused and deterministic outputs.</li>
                        <li><strong>Hallucination:</strong> When an LLM generates text that is plausible-sounding but factually incorrect, nonsensical, or not grounded in the provided context.</li>
                        <li><strong>Delimiter:</strong> Characters or strings (e.g., <code>###</code>, <code>"""</code>, <code>&lt;tag&gt;</code>) used to clearly separate different parts of a prompt (like instructions, context, and questions).</li>
                        <li><strong>Instruction Prompting:</strong> Directly telling the LLM what task to perform using clear action verbs and specific instructions.</li>
                        <li><strong>RAG (Retrieval Augmented Generation):</strong> An approach where an LLM's knowledge is augmented by retrieving relevant information from an external knowledge base (e.g., a set of documents, a database) before generating a response. This helps ground answers in specific, up-to-date, or proprietary information.</li>
                        <li><strong>Token:</strong> The basic unit of text that LLMs process. A token can be a word, part of a word, or punctuation. Prompt length and context windows are often measured in tokens.</li>
                        <li><strong>Fine-tuning:</strong> The process of further training a pre-trained LLM on a smaller, domain-specific dataset to adapt its knowledge and behavior for particular tasks or styles. (This is typically done by developers, not end-users via prompting).</li>
                    </ul>
                `
            },
            {
                id: "further-resources",
                title: "Further Resources",
                content: `
                    <h2>Continue Your Learning Journey</h2>
                    <p>The field of prompt engineering is rapidly evolving. Here are some types of resources where you can find more in-depth information, advanced techniques, and stay updated:</p>
                    <ul>
                        <li><strong>Official Documentation from LLM Providers:</strong>
                            <ul>
                                <li>OpenAI (for ChatGPT, GPT-4) often has best practices and API documentation.</li>
                                <li>Google AI (for Gemini) provides guides and examples.</li>
                                <li>Anthropic (for Claude) shares usage guidelines.</li>
                            </ul>
                        </li>
                        <li><strong>Dedicated Prompt Engineering Guides:</strong> Websites like "PromptingGuide.ai" offer comprehensive tutorials and examples of various techniques.</li>
                        <li><strong>Research Papers:</strong> Academic sites like arXiv.org host many of the original research papers that introduce new prompting techniques (e.g., papers on Chain-of-Thought, Self-Consistency). Search for terms like "prompt engineering," "LLM prompting techniques."</li>
                        <li><strong>Online Communities & Forums:</strong> Platforms like Reddit (e.g., r/PromptEngineering, r/ChatGPT), Discord servers, and specialized forums often have active discussions, shared prompts, and community support.</li>
                        <li><strong>Blogs and Articles from AI Experts & Practitioners:</strong> Many AI researchers and developers share insights and practical tips on their blogs or platforms like Medium, LinkedIn, or Twitter/X.</li>
                        <li><strong>Online Courses:</strong> Platforms like Coursera, Udemy, DeepLearning.AI, and others are increasingly offering courses specifically on prompt engineering or generative AI that include prompting modules.</li>
                    </ul>
                    <div class="note-box">
                        <p><strong>A Key Tip:</strong> The best way to learn is by doing! Experiment with different LLMs, try out the techniques in this primer, and iteratively refine your prompts based on the results you get. Pay attention to what works well for your specific needs and the types of tasks you perform most often.</p>
                    </div>
                    <p>Happy Prompting!</p>
                `
            }
        ];

        const sidebarNav = document.getElementById('sidebarNav');
        const contentArea = document.getElementById('contentArea');
        const menuToggle = document.getElementById('menuToggle');
        const sidebar = document.getElementById('sidebar');
        const prevButton = document.getElementById('prevButton');
        const nextButton = document.getElementById('nextButton');
        const toastNotification = document.getElementById('toastNotification');

        let currentSectionIndex = 0;

        function showToast(message, type = 'success', duration = 2000) {
            toastNotification.textContent = message;
            toastNotification.className = 'toast show';
            if (type === 'success') {
                toastNotification.classList.add('success');
            } else { 
                toastNotification.classList.add('bg-red-500'); 
            }
            
            setTimeout(() => {
                toastNotification.classList.remove('show');
            }, duration);
        }

        function renderSidebar() {
            sidebarNav.innerHTML = ''; 
            primerData.forEach((section, index) => {
                const link = document.createElement('a');
                link.href = `#${section.id}`;
                link.textContent = section.title;
                link.className = 'sidebar-link';
                if (index === currentSectionIndex) {
                    link.classList.add('active');
                }
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    currentSectionIndex = index;
                    renderContent();
                    renderSidebar(); 
                    if (window.innerWidth < 768) { 
                        sidebar.classList.add('collapsed');
                    }
                });
                sidebarNav.appendChild(link);
            });
        }

        function renderContent() {
            if (currentSectionIndex < 0 || currentSectionIndex >= primerData.length) return;

            const section = primerData[currentSectionIndex];
            contentArea.innerHTML = section.content;
            // Add copy functionality to all <pre> blocks that have a copy button
            document.querySelectorAll('#contentArea pre').forEach(preElement => {
                const copyButton = preElement.querySelector('.copy-code-button');
                if (copyButton) {
                    copyButton.addEventListener('click', () => {
                        // Get text content, attempt to remove "Copy" button text if it was accidentally included
                        let textToCopy = preElement.innerText;
                        if (textToCopy.startsWith("Copy\n")) { // Check if "Copy" and newline is at start
                            textToCopy = textToCopy.substring(5); // Remove "Copy\n"
                        } else if (textToCopy.endsWith("\nCopy")) {
                             textToCopy = textToCopy.substring(0, textToCopy.length - 5);
                        } else if (textToCopy.includes("Copy")) {
                            // More robustly find and remove the button text if it's elsewhere
                            const buttonText = copyButton.innerText;
                            textToCopy = textToCopy.replace(buttonText, '').trim();
                        }

                        navigator.clipboard.writeText(textToCopy.trim()).then(() => {
                            showToast('Copied to clipboard!', 'success');
                        }).catch(err => {
                            console.error('Failed to copy: ', err);
                            showToast('Failed to copy.', 'error');
                        });
                    });
                }
            });
            updateNavButtons();
            contentArea.scrollTop = 0; 
        }
        
        function updateNavButtons() {
            if (currentSectionIndex === 0) {
                prevButton.classList.add('disabled');
                prevButton.onclick = null;
            } else {
                prevButton.classList.remove('disabled');
                prevButton.onclick = () => navigateSection(-1);
            }
            if (currentSectionIndex === primerData.length - 1) {
                nextButton.classList.add('disabled');
                nextButton.onclick = null;
            } else {
                nextButton.classList.remove('disabled');
                nextButton.onclick = () => navigateSection(1);
            }
        }

        function navigateSection(direction) {
            const newIndex = currentSectionIndex + direction;
            if (newIndex >= 0 && newIndex < primerData.length) {
                currentSectionIndex = newIndex;
                renderContent();
                renderSidebar();
            }
        }

        menuToggle.addEventListener('click', () => {
            sidebar.classList.toggle('collapsed');
        });
        
        document.addEventListener('click', function(event) {
            if (window.innerWidth < 768 && !sidebar.contains(event.target) && !menuToggle.contains(event.target) && !sidebar.classList.contains('collapsed')) {
                sidebar.classList.add('collapsed');
            }
        });

        renderSidebar();
        renderContent();

    </script>
    <footer>
        <a href="hub.html">Back to Hub</a>
    </footer>
</body>
</html>
